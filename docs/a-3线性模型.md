# 第3章 线性模型

## 一、一元线性回归

### 1.算法原理

通过【发际线高度】预测【计算机水平】
$$
f(x)=w_1x_1+b
$$
二值离散特征【颜值】（好看：1，不好看：0）
$$
f(x)=w_1x_1+w_2x_2+b
$$


有序的多值离散特征【饭量】（小：1，中：2，大：3）
$$
f(x)=w_1x_1+w_2x_2+w_3x_3+b
$$


无序的多值离散特征【肤色】（黄：[1,0,0]，黑：[0,1,0]，白：[0,0,1]）
$$
f(x)=w_1x_1+w_2x_2+w_3x_3+w_4x_4+w_5x_5+w_6x_6+b
$$

### 2.最小二乘估计

基于均方误差最小化对模型求解的方法称为“最小二乘法”。
$$
E(w,b)=\sum_{i=1}^m(y_i-f(x_i))^2=\sum_{i=1}^m(y_i-wx_i-b)^2
$$

### 3.极大似然估计

用途：估计概率分布的参数值

方法：对于离散型（连续型）随机变量$X$，假设其概率质量函数为$P(x;\theta)$（概率密度函数为$p(x;\theta)$），其中$\theta$为待估计的参数值。现有$x_1,x_2,...,x_n$是来自$X$的n个独立同分布的样本，它们的联合概率是一个关于$\theta$的函数，也称为样本的似然函数。
$$
L(\theta)=\prod_{i=1}^n P(x_i;\theta)
$$
极大似然估计的直观想法：使得观测样本出现概率最大的分布就是待求分布，也就是使得似然函数$L(\theta)$取到最大值的$\theta^\prime$即为$\theta$的估计值。

#### 3.1极大似然估计求正态分布中的$\mu,\sigma$

现有一批观测样本$x_1,x_2,...,x_n$，假设其服从正态分布$X\sim N(\mu,\sigma^2)$。

1. 写出随机变量$X$的概率密度函数

$$
p(x;\mu,\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}(-\frac{(x-\mu)^2}{2\sigma^2})
$$

2. 写出似然函数

$$
L(\mu,\sigma^2)=\prod_{i=1}^n p(x_i;\mu,\sigma^2)=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2})
$$

3. 求使得$L(\mu,\sigma^2)$取得最大值的$\mu,\sigma$

由于对数函数ln是单调递增函数，所以$\ln L(\mu,\sigma^2)$和$L(\mu,\sigma^2)$有相同的最大值，利用对数函数可以将连乘项简化为求和，通常把$\ln L(\mu,\sigma^2)$称为对数似然函数。
$$
\ln L(\mu,\sigma^2)=\ln[\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2})]
\\=\sum_{i=1}^n\ln\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(x_i-\mu)^2}{2\sigma^2})
$$


#### 3.2极大似然估计求解线性回归中的w和b

对于线性回归，可以假设其为以下模型
$$
y=wx+b+\epsilon
$$
其中$\epsilon$是不受控制的随机误差，通常假设其服从均值为0的正态分布$\epsilon \sim N(0,\sigma^2)$，所以$\epsilon$的概率密度函数为
$$
p(\epsilon)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{\epsilon^2}{2\sigma^2})
$$
将$\epsilon$用$y-(wx+b)$替换可得
$$
p(y)=\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y-(wx+b))^2}{2\sigma^2})
$$
上式可以看作$y \sim N(wx+b,\sigma^2)$，下面便可以用极大似然估计来估计w和b的值
$$
L(w,b)=\prod_{i=1}^m p(y_i)=\prod_{i=1}^{m}\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y_i-(wx_i+b))^2}{2\sigma^2})
\\
\ln L(w,b)=\sum_{i=1}^m\ln\frac{1}{\sqrt{2\pi}\sigma}\exp(-\frac{(y_i-(wx_i+b))^2}{2\sigma^2})
\\=\sum_{i=1}^m\ln\frac{1}{\sqrt{2\pi}\sigma}+\sum_{i=1}^m\ln\exp(-\frac{(y_i-(wx_i+b))^2}{2\sigma^2})
\\=m\ln\frac{1}{\sqrt{2\pi}\sigma}-\frac{1}{2\sigma^2}\sum_{i=1}^m(y_i-wx_i-b)^2
$$
其中$w,\sigma$均为常数，所以最大化$\ln L(w,b)$等价于最小化$\sum_{i=1}^m(y_i-wx_i-b)^2$，即
$$
(w^*,b^*)=\mathop{\arg\max}\limits_{(w,b)}\ln L(w,b)=\mathop{\arg\max}\limits_{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2
$$
求解w和b本质上是一个多元函数求最值的问题，更具体点就是凸函数求最值的问题。

## 4.概念补充

#### 4.1 凸集

定义：设集合$D\subset\mathbb{R}^n$，如果对任意$x,y\in D$与任意$\alpha\in[0,1]$，有
$$
\alpha x+(1-\alpha)y\in D
$$
则称集合D是凸集。常见凸集有空集$\varnothing$，n维欧式空间$\mathbb{R}^n$。

凸集的几何意义是：若两个点属于此集合，则这两点连线上的任意一点也属于此集合。

#### 4.2 凸函数

定义：设$D$是非空凸集，$f$是定义在$D$上的函数，如果对于任意的$x_1,x_2\in D, \alpha\in(0,1)$，均有
$$
f(\alpha x_1+(1-\alpha)x_2)\leq\alpha f(x_1)+(1-\alpha)f(x_2)
$$
则称$f$为$D$上的凸函数。

#### 4.3 梯度（多元函数的一阶导数）

定义：设n元函数$f(\mathbf{x})$对自变量$\mathbf{x}=(x_1,x_2,...,x_n)^T$的各分量$x_i$的偏导数$\frac{\partial{f(\mathbf{x})}}{\partial{x_i}}(i=1,2,...,n)$都存在，则称$f(\mathbf{x})$在$\mathbf{x}$处一阶可导，并称向量
$$
\nabla f(\mathbf{x})=\begin{bmatrix}
\frac{\partial{f(\mathbf{x})}}{\partial{x_1}}\\
\frac{\partial{f(\mathbf{x})}}{\partial{x_2}}\\...\\
\frac{\partial{f(\mathbf{x})}}{\partial{x_i}}
\end{bmatrix}
$$
为函数$f(\mathbf{x})$在$\mathbf{x}$处的一阶导数或梯度。

#### 4.4 Hessian矩阵（多元函数的二阶导数）

定义：设n元函数$f(x)$对自变量$\mathbf{x}=(x_1,x_2,...,x_n)^T$的各分量$x_i$的二阶偏导数$\frac{\partial{f(\mathbb{x})}}{\partial{x_i}\partial{x_j}}(i=1,2,...,n;j=1,2,...,n)$都存在，则称函数$f(\mathbf{x})$在$\mathbf{x}$处二阶可导，并称矩阵
$$
\nabla^2 f(\mathbf{x})=\begin{bmatrix}
\frac{\partial^2{f(\mathbb{x})}}{\partial{x_1}^2}
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_1}\partial{x_2}}&\cdots
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_1}\partial{x_n}}\\
\frac{\partial^2{f(\mathbb{x})}}{\partial{x_2}\partial{x_1}}
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_2}^2}&\cdots
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_2}\partial{x_n}}\\
\vdots&\vdots&\ddots&\vdots\\
\frac{\partial^2{f(\mathbb{x})}}{\partial{x_n}\partial{x_1}}
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_n}\partial{x_2}}&\cdots
&\frac{\partial^2{f(\mathbb{x})}}{\partial{x_n}^2}
\end{bmatrix}
$$
为函数$f(\mathbf{x})$在$\mathbf{x}$处的Hessian（海塞）矩阵。

#### 4.5 半正定矩阵

定义：设A是n阶方阵，如果对于任何非零向量X，都有$X^{T}AX\geq 0$，则称A是半正定矩阵。
