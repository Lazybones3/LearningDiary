# 第6章 支持向量机

## 一、算法原理

从几何角度，对于线性可分数据集，支持向量机就是找距离正负样本都最远的超平面，相比于感知机，其解是唯一的，且不偏不倚，泛化性能更好。

## 二、超平面

n维向量的超平面($w^Tx+b=0$​​，其中$w,x\in\mathbb{R}^n$​)

- 超平面方程不唯一

- 法向量w和位移项b确定一个唯一超平面

- 法向量w垂直于超平面（缩放w,b时，若缩放倍数为负数会改变法向量方向）

- 法向量w指向的那一半空间为正空间，另一半为负空间

- 任意点x到超平面的距离公式为
  $$
  r=\frac{|w^Tx+b|}{||w||}
  $$
  

## 三、几何间隔

对于给定的数据集X和超平面$w^Tx+b=0$，定义数据集X中的任意一个样本点$(x_i,y_i),y\in{-1,1},i=1,2,...,m$关于超平面的几何间隔为
$$
\gamma_i=\frac{y_i(w^Tx_i+b)}{||w||}
$$
正确分类时：$\gamma_i>0$，几何间隔此时也等价于点到超平面的距离

没有正确分类时：$\gamma_i<0$

对于给定的数据集X和超平面$w^T+b=0$​，定义数据集X关于超平面的几何间隔为：数据集X中所有样本点的几何间隔最小值
$$
\gamma=\mathop{\min}\limits_{i=1,2,...,m}\gamma_i
$$

## 四、支持向量机

模型：给定线性可分数据集X，支持向量机模型希望求得数据集X关于超平面的几何间隔$\gamma$达到最大的那个超平面，然后套上一个sign函数实现分类功能
$$
y=sign(w^Tx+b)=\begin{cases}
1,w^Tx+b>0\\
-1,w^T+b<0
\end{cases}
$$
所以其本质和感知机一样，仍然是求一个超平面。

定义在满足$\mu\succeq0$这个约束条件下求对偶函数最大值的优化问题为拉格朗日对偶问题​
$$
\max\quad\Gamma(\mu,\lambda)\\
s.t.\quad\mu\succeq0
$$
设该优化问题的最优值为$d^*$，显然$d^*\leq p^*$，此时称为“弱对偶性”成立，若$d^*=p^*$​​，则称为“强对偶性”成立。

- 当主问题满足某些充分条件时，强对偶性成立。常见的充分条件有Slater条件：若主问题是凸优化问题，且可行集$\tilde{D}$中存在一点能使所有不等式约束的不等号成立，则强对偶性成立。显然支持向量机满足Slater条件。
- 无论主问题是否为凸优化问题，对偶问题恒为凸优化问题，因为对偶函数$\Gamma(\mu,\lambda)$恒为凹函数，约束条件$\mu\succeq 0$恒为凸集。
- 设$f(x),g_i(x),h_i(x)$一阶偏导连续，$x^*,(\mu^*,\lambda^*)$分别为主问题和对偶问题的最优解，若强对偶性成立则$x^*,\mu^*,\lambda^*$​满足KKT条件。

## 五、支持向量回归

相比于线性回归用一条线拟合训练样本，支持向量回归(SVR)采用一个以$f(x)=w^Tx+b$为中心，宽度为$2\epsilon$的间隔带，来拟合训练样本。

SVR的优化问题可以写为
$$
\mathop{\min}_{w,b}\frac{1}{2}||w||^2+C\sum_{i=1}^ml_{\epsilon}(f(x_i)-y_i)
$$
其中$l_{\epsilon}(z)$​为$\epsilon$不敏感损失函数
$$
l_{\epsilon}(z)=\begin{cases}
0,|z|\leq\epsilon\\
|z|-\epsilon,|z|>\epsilon
\end{cases}
$$
