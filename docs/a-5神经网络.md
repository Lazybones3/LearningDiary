# 第5章 神经网络

## 一、M-P神经元

M-P神经元：接收n各输入（通常来自其他神经元），并给各个输入赋予权重计算加权和，然后和自身特有的阈值$\theta$进行比较（做减法），最后进过激活函数处理得到输出（通常是给下一个神经元）
$$
y=f(\sum_{i=1}^nw_ix_i-\theta)=f(w^Tx+b)
$$
单个M-P神经元：感知机（sgn作激活函数）、对数几率回归（sigmoid作激活函数）

多个M-P神经元：神经网络

## 二、感知机

1. 感知机模型：激活函数为sgn（阶跃函数）的神经元

$$
y=sgn(w^Tx-\theta)=\begin{cases}
1,w^Tx-\theta\geq0\\
0,w^Tx-\theta<0
\end{cases}
$$

其中$x\in\mathbb{R}^n$为样本的特征向量，是感知机的输入，$w,\theta$​是感知机的参数，$w\in\mathbb{R}^n$为权重，$\theta$为阈值。

再从几何角度来说，给定一个线性可分的数据集T，感知机的学习目标是求得能对数据集T中的正负样本完全正确划分的超平面，其中$w^Tx-\theta$即为超平面方程。

n维空间的超平面($w^Tx+b=0$​，其中$w,x\in\mathbb{R}^n$​​)：

- 超平面方程不唯一
- 法向量w垂直于超平面
- 法向量w和位移项b确定一个唯一超平面
- 法向量w指向的那一半空间为正空间，另一半为负空间

2. 感知机学习策略：随机初始化w,b，将全体训练样本代入模型找出误分类样本，假设此时误分类样本集合为$M\subseteq T$​，对任意一个误分类样本$(x,y)\in M$，当$w^Tx-\theta \geq0$时，模型输出值为$\hat{y}=1$，样本真实标记为$y=0$；反之，当$w^Tx-\theta<0$时，模型输出值为$\hat{y}=0$，样本真实标记为$y=1$​。综合两种情形可知，以下公式恒成立
   $$
   (\hat{y}-y)(w^Tx-\theta)\geq0
   $$
   所以给定数据集T，其损失函数为：
   $$
   L(w,\theta)=\sum_{x\in M}(\hat{y}-y)(w^Tx-\theta)
   $$
   显然此损失函数是非负的。如果没有误分类点，损失函数值是0。误分类点越少，误分类点离超平面越近，损失函数值就越小。

3. 感知机学习算法：当误分类样本集合M固定时，那么可以求得损失函数$L(w)$的梯度为
   $$
   \nabla_{w}L(w)=\sum_{x_i\in M}(\hat{y}_i-y_i)x_i
   $$
   感知机的学习算法采用的是随机梯度下降法，也就是极小化过程中不是一次使用M中所有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。所以权重w的更新公式为
   $$
   w\leftarrow w+\Delta w\\
   \Delta w=-\eta(\hat{y}_i-y_i)x_i=\eta(y_i-\hat{y}_i)x_i
   $$

## 三、神经网络

1. 多层前馈网络：每层神经元与下一层神经元全互连，神经元之间不存在同层连接，也不存在跨层连接。

   将神经网络（记作NN）看作一个特征加工函数：
   $$
   x\in\mathbb{R}_d\rightarrow NN(x)\rightarrow y=x^*\in\mathbb{R}^l
   $$
   回归：后面接一个$\mathbb{R}^l\rightarrow\mathbb{R}$的神经元，例如：没有激活函数的神经元
   $$
   y=w^Tx^*+b
   $$
   分类：后面接一个$\mathbb{R}^l\rightarrow[0,1]$的神经元，例如：激活函数为sigmoid函数的神经元
   $$
   y=\frac{1}{1+e^{-(w^Tx^*+b)}}
   $$

2. 误差逆传播算法（BP算法）：基于随机梯度下降的参数更新算法
   $$
   w\leftarrow w+\Delta w\\
   \Delta w=-\eta\nabla_w E
   $$
   其中只需推导出$\nabla_w E$​这个损失函数E关于参数w的一阶偏导数。由于NN(x)通常是及其复杂的非凸函数，不具备像凸函数的性质，因此随机梯度下降不能保证一定能走到全局最小点，更多情况下走到的都是局部极小值点。

