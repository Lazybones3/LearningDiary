# 第4章 决策树

## 一、ID3决策树

1. 自信息：

$$
I(X)=-\log_b p(x)
$$

当b=2时单位为bit，当b=e时单位为nat

2. 信息熵（自信息的期望）：度量随机变量$X$的不确定性，信息熵越大越不确定
$$
  H(X)=E[I(X)]=-\sum_x p(x)\log_b p(x)
$$

此处以离散型随机变量为例，计算信息熵时约定：若$p(x)=0$，则$p(x)\log_b p(x)=0$

当$X$​的某个取值概率为1时信息熵最小（最确定），其值为0；当$X$的各个取值的概率均等时信息熵最大（最不确定），其值为$\log_b|X|$，其中$|X|$表示$X$可能取值的个数。

3. 将样本类别标记y视为随机变量，各个类别在样本集合D中的占比$p_k(k=1,2,...,|\gamma|)$​视为各个类别取值的概率，则样本集合D（随机变量y）的信息熵（底数b取2）为

$$
Ent(D)=-\sum_{k=1}^{|\gamma|}p_k\log_2p_k
$$

此时的信息熵所代表的“不确定性”可以理解为集合内样本的“纯度”

4. 条件熵（Y的信息熵关于概率分布X的期望）：在已知X后Y的不确定性

$$
H(Y|X)=\sum_xp(x)H(Y|X=x)
$$

从单个属性a的角度来看，假设其可能取值为$\{a^1,a^2,...,a^V\}$​，$D^v$​表示属性取值为$a^v\in\{a^1,a^2,...,a^V\}$​​​的样本集合，$\frac{D^v}{D}$表示占比，那么在已知属性$a$的取值后，样本集合$D$的条件熵为
$$
\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$

5. 信息增益：在已知属性a的取值后y的不确定性减少的量，即纯度的提升

$$
Gain(D,a)=Ent(D)-\sum_{v=1}^V\frac{|D^v|}{|D|}Ent(D^v)
$$

上式中的信息增益=信息熵-条件熵

6. ID3决策树：以信息增益为准则来选择划分属性的决策树

$$
a_*=\mathop{\arg\max}\limits_{a\in A}Gain(D,a)
$$

## 二、C4.5决策树

1. 信息增益准则对可能取值数目较多的属性有所偏好，为减少这种偏好可能带来的不利影响，C4.5决策树选择使用“增益率”代替“信息增益”，增益率定义为

$$
GainRatio(D,a)=\frac{Gain(D,a)}{IV(a)}
$$
其中
$$
IV(a)=-\sum_{v=1}^{V}\frac{|D^v|}{|D|}\log_2\frac{|D^v|}{|D|}
$$

成为属性a的“固有值”，a的可能取值个数V越大，通常其固有值$IV(a)$也越大。但是增益率对可能取值数目较少的属性有所偏好。

因此C4.5决策树并未完全使用“增益率”代替“信息增益”，而是采用一种启发式的方法：先选出信息增益高于平均水平的属性，然后再从中选择增益率最高的。

## 三、CART决策树

1. 基尼值：从样本集合D中随机抽取两个样本，其类别标记不一致的概率。

基尼值越小，碰到异类的概率就越小，纯度自然就越高。
$$
Gini(D)=\sum_{k=1}^{|\gamma|}\sum_{k^\prime\neq k}p_kp_{k^\prime} \\
=\sum_{k=1}^{|\gamma|}p_k(1-p_k) \\
=1-\sum_{k=1}^{|\gamma|}p_k^2
$$
属性a的基尼指数（类比信息熵和条件熵）：
$$
GiniIndex(D,a)=\sum_{v=1}^V\frac{|D^v|}{D}Gini(D^v)
$$

2. CART决策树：选择基尼指数最小的属性作为最优划分属性

$$
a_*=\mathop{\arg\min}\limits_{a\in A}GiniIndex(D,a)
$$

CART决策树的实际构造算法如下：

- 首先，对每个属性a的每个可能取值v，将数据集D分为$a=v$和$a\neq v$两部分来计算基尼指数，即
  $$
  GiniIndex(D,a)=\frac{|D^{a=v}|}{|D|}Gini(D^{a=v})+\frac{|D^{a\neq v}|}{|D|}Gini(D^{a\neq v})
  $$
  
- 然后，选择基尼指数最小的属性及其对应取值作为最优划分属性和最优划分点。
- 最后，重复以上两步，直到满足停止条件。
