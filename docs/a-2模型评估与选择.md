# 第2章 模型评估与选择

## 2.1 经验误差与过拟合

- 如果在m个样本中有a个样本分类错误，则错误率$E=a/m$，精度=1-错误率。
- 误差（error）：学习器在的实际预测输出与样本的真实输出之间的差异。
- 训练误差（training error）或经验误差（empirical error）：学习器在训练集上的误差。
- 泛化误差（generalization error）：学习器在新样本上的误差。
- 过拟合（overfitting）：当学习器把训练样本学得“太好了”的时候，有可能把训练样本自身的一些特点当作了所有潜在样本都具有的一般性质，这样就会导致泛化性能下降，这种现象就称为过拟合。
- 欠拟合（underfitting）：学习器对训练样本的一般性质尚未学好。

## 2.2 评估方法

### 2.2.1 留出法

留出法（hold-out）：将数据集D划分为两个互斥的集合，其中一个作为训练集S，另一个作为测试集T，在S上训练出模型后，用T来评估其测试误差。

分层抽样（stratified sampling）：保留类别比例的采样方式。

### 2.2.2 交叉验证法

交叉验证法（cross validation）：将数据集D划分为k个大小相似的互斥子集，然后每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就可获得k组训练/测试集，从而可进行k次训练和测试，最终返回这k个测试结果的均值，通常把这称为k折交叉验证（k-fold cross validation）。

### 2.2.3 自助法

自助法（bootstrapping）：给定包含m个样本的数据集D，我们对它采样产生数据集D1：每次随机从D中挑选一个样本放入D1，然后将样本放回D中，这样重复执行m次，得到包含m个样本的数据集D1。于是可将D1作为训练集，D\D1作为测试集（\表示集合减法）。

### 2.2.4 调参与最终模型

测试集：用于估计模型在实际使用时的泛化能力。

验证集（validation set）：在模型选择与调参中用于评估测试的数据集。

## 2.3 性能度量

性能度量（performance measure）是衡量模型泛化能力的评价标准。

回归任务最常用的性能度量是均方误差（mean squared error）。

给定样例集 $D={(x_1,y_1),(x_2,y_2),...,(x_m,y_m)}$，其中$y_i$是示例$x_i$的真实标记。
$$
E(f;D)=\frac{1}{m}\sum_{i=1}^m(f(x_i)-y_i)^2
$$


### 2.3.1 错误率与精度

错误率与精度是分类任务中最常用的两种性能度量。

- 错误率

$$
E(f;D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(f(x_i)\neq y_i)
$$

- 精度

$$
acc(f;D)=\frac{1}{m}\sum_{i=1}^m\mathbb{I}(f(x_i)=y_i)=1-E(f;D)
$$

### 2.3.2 查准率、查全率与F1

- 混淆矩阵

<table>
	<tr>
    <td rowspan="2">真实情况</td>
    <td colspan="2">预测结果</td>
  </tr>
  <tr>
    <td>正例</td>
    <td>反例</td>
  </tr>
  <tr>
    <td>正例</td>
    <td>TP（真正例）</td>
    <td>FN（假反例）</td>
  </tr>
  <tr>
    <td>反例</td>
    <td>FP（假正例）</td>
    <td>TN（真反例）</td>
  </tr>
</table>

- 查准率P：TP占预测结果的比例

$$
P=\frac{TP}{TP+FP}
$$

- 查全率R：TP占真实情况的比例

$$
R=\frac{TP}{TP+FN}
$$

- P-R曲线：以查准率为纵轴，查全率为横轴。平衡点（Break-Event Point）是查准率=查全率时的取值，平衡点取值越大，性能越好。
- F1是基于查准率与查全率的调和平均

$$
F1=\frac{2 \times P \times R}{P+R}=\frac{2 \times TP}{样例总数+TP-TN}
$$
